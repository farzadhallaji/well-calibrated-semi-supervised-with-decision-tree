{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = {'1':'r', '2':'g', 'Iris-virginica':'b'}\n",
    "\n",
    "# # fig, ax = plt.subplots()\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d') \n",
    "# for i in range(len(X_train)):\n",
    "#      ax.scatter(X_train[i][5] ,X_train[i][1], X_train[i][2],color=colors[Y_train[i]] , s=50, alpha=0.6, edgecolors='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library ðŸ˜š\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "from scipy.io.arff import loadarff\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config ðŸ˜š\n",
    "path_to_datasets = '/home/farzad/Desktop/semiWithTree/originDataset/'\n",
    "dataset_name = 'bupa'\n",
    "\n",
    "\n",
    "dataset_path  = path_to_datasets + dataset_name\n",
    "base_classifier = DecisionTreeClassifier\n",
    "random_state = 0\n",
    "min_samples_leaf=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_xy(test_data):\n",
    "    # assert : class = last atr ðŸ˜š\n",
    "    x_test = test_data.values[:, 0:-1]\n",
    "    y_test = (test_data.values[:, -1]).astype('int')\n",
    "    \n",
    "    return x_test,y_test\n",
    "\n",
    "def read_data(dataset_path) :\n",
    "    \n",
    "    train_raw_data = loadarff(dataset_path+'/train.arff')\n",
    "    test_raw_data = loadarff(dataset_path+'/test.arff')\n",
    "    \n",
    "    train_data = pd.DataFrame(train_raw_data[0])\n",
    "    test_data = pd.DataFrame(test_raw_data[0])\n",
    "    \n",
    "    train_data['Class'] = train_data['Class'].astype(int)\n",
    "    test_data['Class'] = test_data['Class'].astype(int)\n",
    "    \n",
    "    return train_data,test_data\n",
    "\n",
    "\n",
    "def get_rate_p(train_y) : \n",
    "    \n",
    "    counter=collections.Counter(train_y)\n",
    "    tuple_list_pn = counter.most_common()\n",
    "    \n",
    "    return tuple_list_pn[0][1]/(tuple_list_pn[0][1]+tuple_list_pn[1][1]) , tuple_list_pn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int([1 , 2.0])\n",
    "# # int(b'1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data,test_data = read_data(dataset_path)\n",
    "# # (train_x,train_y),(test_x , test_y) \n",
    "# counter=collections.Counter(train_y)\n",
    "# a=counter.most_common()\n",
    "# a[0][1]/a[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_trainset(train_data) :\n",
    "    \n",
    "    labeled , unlabeled = [],[]\n",
    "    \n",
    "    size_dataset = len(train_data)\n",
    "    train_x,train_y = divide_xy(train_data)\n",
    "    \n",
    "    rate_p , tuple_list_pn = get_rate_p(train_y)\n",
    "    \n",
    "    size_labeled_data = round(0.1 * size_dataset)\n",
    "    size_unlabeled_data = size_dataset - size_labeled_data\n",
    "    \n",
    "    size_labeled_p_data = round(rate_p*size_labeled_data)\n",
    "    size_labeled_n_data = size_labeled_data - size_labeled_p_data\n",
    "    \n",
    "    labeled_index = []\n",
    "    unlabeled_index = []\n",
    "    selected_pl = 0\n",
    "    selected_nl = 0\n",
    "    \n",
    "    for i,cls in enumerate(train_y):\n",
    "        # if data point class's == 0 ðŸ˜š\n",
    "        if cls == tuple_list_pn[0][0] :\n",
    "            if selected_pl < size_labeled_p_data :\n",
    "                labeled_index.append(i)\n",
    "                selected_pl+=1\n",
    "            else :\n",
    "                unlabeled_index.append(i)\n",
    "        else :\n",
    "            if selected_nl < size_labeled_n_data :\n",
    "                labeled_index.append(i)\n",
    "                selected_nl+=1\n",
    "            else :\n",
    "                unlabeled_index.append(i)\n",
    "                \n",
    "    for i in labeled_index:\n",
    "        labeled.append(train_data.values[i])\n",
    "    \n",
    "    for i in unlabeled_index:\n",
    "        unlabeled.append(train_data.values[i])\n",
    "    \n",
    "#     print(size_dataset , size_labeled_data , size_unlabeled_data)\n",
    "#     print(rate_p , tuple_list_pn)\n",
    "#     print(size_labeled_p_data , size_labeled_n_data)\n",
    "#     print(selected_pl/(selected_pl+selected_nl),selected_pl, selected_nl)\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(labeled,columns=train_data.columns),pd.DataFrame(unlabeled,columns=train_data.columns),rate_p,tuple_list_pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(base_classifier, labeled_data, test_data):\n",
    "\n",
    "    labeled_x,labeled_y = divide_xy(labeled_data)\n",
    "    test_x,test_y = divide_xy(test_data)\n",
    "\n",
    "    # dtree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "    dtree=base_classifier(random_state = random_state, min_samples_leaf=min_samples_leaf)\n",
    "    dtree.fit(labeled_x,labeled_y)\n",
    "    \n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(dtree, out_file=dot_data,filled=True, rounded=True,special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    img = Image(graph.create_png())\n",
    "    \n",
    "    y_pred = dtree.predict(test_x)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(test_y, y_pred)\n",
    "    \n",
    "    return accuracy , img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data,test_data = read_data(dataset_path)\n",
    "# labeled_data , unlabeled_data , rate_p,tuple_list_pn = split_trainset(train_data)\n",
    "# # (train_x,train_y),(test_x , test_y) \n",
    "# unlabeled_data.at[1, 'Class'] = 0\n",
    "# pd.concat([labeled_data , pd.DataFrame([unlabeled_data.values[1]] , columns=labeled_data.columns)] , ignore_index=True)\n",
    "# # labeled_data\n",
    "# # unlabeled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def confidency(name , DTclassifier , labeled_data , unlabeled_data , i , confidence) :\n",
    "\n",
    "    test_x,test_y = divide_xy(unlabeled_data)\n",
    "    is_confident = False\n",
    "    lbl = DTclassifier.predict([test_x[i]])\n",
    "    if name == 'Probability' :\n",
    "        i_confidence = DTclassifier.predict_proba([test_x[i]])\n",
    "        if max(i_confidence[0]) > confidence :\n",
    "            is_confident = True\n",
    "    \n",
    "    return is_confident , lbl\n",
    "\n",
    "\n",
    "def selection_metric(labeled_data,unlabeled_data ,rate_p,tuple_list_pn , confidence,selection_rate , confidence_method_name) :\n",
    "    \n",
    "    labeled_x,labeled_y = divide_xy(labeled_data)\n",
    "    unlabeled_x,unlabeled_y = divide_xy(unlabeled_data)\n",
    "\n",
    "    # dtree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "    DTclassifier = base_classifier(random_state = random_state, min_samples_leaf=min_samples_leaf)\n",
    "    DTclassifier.fit(labeled_x,labeled_y)\n",
    "    \n",
    "    removed_selected_data = unlabeled_data.copy()\n",
    "    total_selected_labeling = pd.DataFrame(columns=labeled_data.columns)\n",
    "    selected_labeling = pd.DataFrame(columns=labeled_data.columns)\n",
    "    selected_index = []\n",
    "    selected_y = []\n",
    "    \n",
    "    \n",
    "    for i  in range(len(removed_selected_data)) :\n",
    "        is_confident , lbl = confidency(confidence_method_name , DTclassifier , labeled_data ,\n",
    "                                        removed_selected_data , i , confidence)\n",
    "        if is_confident: \n",
    "            selected_index.append(i)\n",
    "            selected_y.append(lbl[0])\n",
    "            # set class\n",
    "            removed_selected_data.at[i, 'Class'] = lbl[0]\n",
    "            \n",
    "    #should be constant rate \n",
    "#     print(np.array(selected_y))\n",
    "    new_rate_p,new_tuple_list_pn = get_rate_p(np.array(selected_y))\n",
    "    selected_index_p = []\n",
    "    selected_index_n = []\n",
    "    size_selected  = selection_rate * len(labeled_data)\n",
    "    size_select_p = size_selected * rate_p\n",
    "    \n",
    "    if(size_select_p > new_tuple_list_pn[0][1] ) : \n",
    "        raise Exception(\"size_select_p > new_tuple_list_pn[0][1] \\n change selection_rate \") \n",
    "        \n",
    "    p = new_tuple_list_pn[0][0]\n",
    "    for i in range(len(selected_y)):\n",
    "        if size_selected == 0 :\n",
    "            break\n",
    "        if selected_y[i] == p :\n",
    "            selected_index_p.append(i)\n",
    "        else :\n",
    "            selected_index_n.append(i)\n",
    "            \n",
    "        size_selected-=1\n",
    "        \n",
    "        \n",
    "    for i in range(len(selected_index_p)):\n",
    "        selected_labeling =pd.concat([selected_labeling ,\n",
    "                                      pd.DataFrame([removed_selected_data.values[i]],\n",
    "                                                   columns=labeled_data.columns)],    ignore_index=True)\n",
    "    for i in range(len(selected_index_n)):\n",
    "        selected_labeling =pd.concat([selected_labeling ,\n",
    "                                      pd.DataFrame([removed_selected_data.values[i]],\n",
    "                                                   columns=labeled_data.columns)],    ignore_index=True)\n",
    "\n",
    "    removed_selected_data.drop(removed_selected_data.index[selected_index])\n",
    "    \n",
    "    total_selected_labeling = pd.concat([labeled_data ,selected_labeling],ignore_index=True)\n",
    "    \n",
    "    return total_selected_labeling,removed_selected_data\n",
    "\n",
    "\n",
    "def self_labeling(labeled_data , unlabeled_data , iteration , rate_p,tuple_list_pn , confidence,selection_rate,confidence_method_name):\n",
    "\n",
    "    \n",
    "    labeled_unlabel_data = labeled_data.copy()\n",
    "    removed_selected_data = unlabeled_data.copy()\n",
    "    \n",
    "    while iteration:\n",
    "        \n",
    "        selected_labeling,removed_selected_data = selection_metric(labeled_unlabel_data,removed_selected_data,\n",
    "                                                                   rate_p,tuple_list_pn ,\n",
    "                                                                   confidence,selection_rate,\n",
    "                                                                   confidence_method_name)\n",
    "        labeled_unlabel_data = pd.concat([labeled_unlabel_data , selected_labeling])\n",
    "        \n",
    "        print('iteration:' , iteration , ' , selected_labeling:' , len(selected_labeling))\n",
    "        iteration-=1\n",
    "        \n",
    "    return labeled_unlabel_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \n",
    "    train_data,test_data = read_data(dataset_path)\n",
    "    train_x,train_y = divide_xy(train_data)\n",
    "    test_x , test_y = divide_xy(test_data)\n",
    "    \n",
    "    labeled_data,unlabeled_data , rate_p,tuple_list_pn = split_trainset(train_data)\n",
    "    \n",
    "    a1 , img1 = evaluate_classifier(base_classifier , labeled_data  , test_data)\n",
    "    \n",
    "    total_labeled_data = self_labeling(labeled_data , unlabeled_data , 1, \n",
    "                                       rate_p,tuple_list_pn ,\n",
    "                                       confidence=0.9,selection_rate = 1,confidence_method_name='Probability')\n",
    "    \n",
    "    a2 , img2 = evaluate_classifier(base_classifier , total_labeled_data  , test_data)\n",
    "    \n",
    "    print(a1 , a2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), 1, array([[0., 1.]]))"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data,test_data = read_data(dataset_path)\n",
    "labeled_data , unlabeled_data , rate_p,tuple_list_pn = split_trainset(train_data)\n",
    "# (train_x,train_y),(test_x , test_y) \n",
    "\n",
    "# a , b = evaluate_classifier(base_classifier , labeled_data  , test_data)\n",
    "# print(a)\n",
    "# # b\n",
    "# # labeled_data.at[0, labeled_data.columns[-1]] = 0\n",
    "# for i  in range(len(labeled_data)) :\n",
    "# #     print(labeled_data.loc[i])\n",
    "#     break\n",
    "    \n",
    "# # labeled_data.add(labeled_data.loc[0])\n",
    "# labeled_data.at[i, 'Class']\n",
    "# # labeled_data.loc[]\n",
    "# # train_data.columns\n",
    "# # test_data.columns\n",
    "\n",
    "\n",
    "labeled_x,labeled_y = divide_xy(labeled_data)\n",
    "unlabeled_x,unlabeled_y = divide_xy(unlabeled_data)\n",
    "\n",
    "# dtree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "DTclassifier = base_classifier(random_state = random_state , min_samples_leaf=min_samples_leaf)\n",
    "DTclassifier.fit(unlabeled_x,unlabeled_y)\n",
    "\n",
    "i=5\n",
    "DTclassifier.predict([labeled_x[i]]) , labeled_y[i] , DTclassifier.predict_proba([labeled_x[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1  , selected_labeling: 46\n",
      "0.6106194690265486 0.5929203539823009\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, [(1, 1), (2, 1)])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rate_p(np.array([1 , 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
